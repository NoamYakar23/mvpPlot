{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2c497059-75e8-43bb-8527-c0eb8bb186b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.52537272e-03, -3.04846304e-02, -5.32764787e-03, -1.20928173e-02,\n",
       "       -1.31701702e-02, -1.19664155e-02,  3.01834595e-02, -4.25313069e-02,\n",
       "       -2.03523861e-01,  1.31765110e-02,  8.11047888e-02, -4.88797937e-02,\n",
       "        8.74867358e-04, -3.09397119e-03, -2.99033946e-03, -1.74007945e-02,\n",
       "       -8.79068813e-01,  2.49090927e-02,  1.20274149e+00,  1.50971457e+00])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "#Newton Optimization Method\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "#Logistic Regression with two parameters (one dimensional vector input) \n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, weightsInit, x, y, maxIters, delta, regTerm):\n",
    "        #this is a numpy array with two inputs, where the first input regulates shift of the sigmoid function, and the second regulates generalized slope of the sigmoid function\n",
    "        #x and y are expected to be numpy arrays with y being binary classification\n",
    "        self.weights = weightsInit\n",
    "        self.maxIters = maxIters\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.delta = delta\n",
    "        self.regularizationTerm = regTerm\n",
    "        \n",
    "        \n",
    "    def sigmoid(self):\n",
    "        #Output of the sigmoid function, utilizing the weights that were previously initialized\n",
    "        z = np.dot(X, self.weights)\n",
    "        \n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def logLikelihood(self):\n",
    "        hyp = self.sigmoid()\n",
    "        logLoss = y*(np.log(hyp)) + (1-y)*(np.log(1-hyp))\n",
    "        return np.sum(logLoss)\n",
    "    \n",
    "    def gradient(self):\n",
    "        #return gradient with respect to each parameter\n",
    "        hyp = self.sigmoid()\n",
    "        error = hyp - y\n",
    "        grad = np.dot(X.T, error)\n",
    "        \n",
    "        return grad\n",
    "        \n",
    "           \n",
    "    def hessian(self):\n",
    "        \n",
    "        hyp = self.sigmoid()\n",
    "        \n",
    "        hDiag = np.diag(hyp*(1-hyp))\n",
    "        hessian = np.dot(X.T, np.dot(hDiag, X))\n",
    "        \n",
    "        #add regularization term to prevent singularity of hessian matrix\n",
    "        n = X.shape[1]\n",
    "        ridge = np.eye(n) + self.regularizationTerm\n",
    "        hessian += ridge\n",
    "        \n",
    "        return hessian\n",
    "    \n",
    "    def newtonianOptimization(self):\n",
    "        i = 0\n",
    "        likelihood = self.logLikelihood()\n",
    "        delLike = np.Infinity\n",
    "        while (abs(delLike) > self.delta and i < self.maxIters):\n",
    "            i+=1\n",
    "            #compute gradient\n",
    "            grad = self.gradient()\n",
    "            \n",
    "            #compute hessian\n",
    "            hesh = self.hessian()\n",
    "\n",
    "            #take inverse of hessian\n",
    "            inverseHesh = np.linalg.inv(hesh)\n",
    "            \n",
    "            change = np.dot(inverseHesh, grad) # Matrix multiplication of inverse hessian and gradient to compute change in parameter\n",
    "            \n",
    "            \n",
    "            self.weights -= change\n",
    "            \n",
    "            newLikelihood = self.logLikelihood()\n",
    "            delLike = likelihood - newLikelihood\n",
    "            likelihood = newLikelihood\n",
    "        return self.weights\n",
    "           \n",
    "            \n",
    "    def fit(self):\n",
    "        #add column of all ones for the intercepts\n",
    "        #np.c_ = np.concatenate\n",
    "        \n",
    "        n_features = self.x.shape[1]\n",
    "        self.weights = np.random.uniform(-0.01, 0.01, n_features)\n",
    "        \n",
    "        self.weights = self.newtonianOptimization()\n",
    "\n",
    "X, y = make_classification(n_samples = 10000, n_features = 20, n_classes = 2, random_state = 99)\n",
    "logReg = LogisticRegression(weightsInit, X, y, 1000, delta = 0.0000001, regTerm = 0.02)\n",
    "logReg.fit()\n",
    "\n",
    "logReg.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee1352b-c1c0-490d-b28f-b6e6cae08cde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9ce5bf-7ab5-42b9-bbf3-bf5777842be4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
